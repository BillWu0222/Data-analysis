library(dplyr)
library(caret)
library(ggplot2)
library(readr)
library(nortest)
## =========================================資料前處理=========================================##
## 1.1
## 設定到本地路徑
setwd("/Users/changchenyu/Desktop/中山大學/巨量資料分析/Group_exercise/a1/")
library(dplyr)
library(caret)
library(ggplot2)
library(readr)
library(nortest)
## =========================================資料前處理=========================================##
## 1.1
## 設定到本地路徑
#setwd("/Users/changchenyu/Desktop/中山大學/巨量資料分析/Group_exercise/a1/")
## 讀入 data
Car_Merge_A <- read.csv("./dataset/Car_Merge_A.csv")
library(readr)
Car_Merge_A <- read_csv("~/Desktop/巨量課/dataset/Car_Merge_A.csv")
View(Car_Merge_A)
library(readr)
Car_Merge_B <- read_csv("~/Desktop/巨量課/dataset/Car_Merge_B.csv")
View(Car_Merge_B)
library(readr)
Car_Concat <- read_csv("~/Desktop/巨量課/dataset/Car_Concat.csv")
View(Car_Concat)
## 1.1
## 設定到本地路徑
#setwd("/Users/changchenyu/Desktop/中山大學/巨量資料分析/Group_exercise/a1/")
## 讀入 data
Car_Merge_A <- read.csv("~/Desktop/巨量課/dataset/Car_Merge_A.csv")
Car_Merge_B <- read.csv("~/Desktop/巨量課/dataset/Car_Merge_B.csv")
Car_Concat <- read.csv("~/Desktop/巨量課/dataset/Car_Concat.csv")
## 合併資料集 A, B
merged_data <- inner_join(Car_Merge_A, Car_Merge_B, by = "car_ID")
## 與 Concat 資料 cancat
final_data <- bind_rows(merged_data, Car_Concat)
## 將 character 欄位轉換成 factor，表示離散類型的資料
character_columns <- sapply(final_data, is.character)
final_data[character_columns] <- lapply(final_data[character_columns], as.factor)
## 移除 ID 欄位
rownames(final_data) <- final_data$car_ID
final_data <- final_data[, !(names(final_data) %in% "car_ID")]
head(final_data)
lapply(colnames(final_data), function(x){
# categorical/factor variables
if(is.factor(final_data[, x] )){
print(x); print(table(final_data[, x]))
qplot(final_data[, x], geom = "bar") + xlab(x)
# continuous variables
}else{
qplot(final_data[, x], geom = "density",ylab = "Density") + xlab(x)
}
})
car_var = setdiff(colnames(final_data), "PRICE")
lapply(car_var, function(x) {
## simple linear model
lm_xy = lm(eval(parse(text = paste("PRICE ~", x))), data = final_data)
if (is.factor(final_data[, x])) {
## for categorical
anova_result <- aov(as.formula(paste("PRICE ~", x)), data = final_data)
print(summary(anova_result))
ggplot(data = final_data, aes_string(x = x, y = "PRICE")) + geom_boxplot() + labs(x = x, y = "PRICE")
# type 3 代表所有自變量的影響
} else {
## scatter for numeric
print(summary(lm_xy))
ggplot(data = final_data, aes_string(x = x, y = "PRICE")) + geom_point() + labs(x = x, y = "PRICE")
}
})
nortest::lillie.test(final_data$PRICE)
#常態分佈檢測 p-value = 8.092e-13 < 0.05, 不符合常態分佈
ad.test(final_data$PRICE)
lm_all <- lm(PRICE ~ ., data = final_data)
summary(lm_all)
# select
selected_vars <- c("wheelbase", "enginesize", "boreratio", "stroke", "compressionratio", "horsepower", "peakrpm", "mpg")
formula <- as.formula(paste("PRICE ~", paste(selected_vars, collapse = " + ")))
lm_select <- lm(formula, data = final_data)
summary(lm_select)
set.seed(20230929)
train_idx = sample(1:nrow(final_data), 0.7 * nrow(final_data))
train_d = final_data[train_idx,]
test_d = final_data[setdiff(1:nrow(final_data), train_idx),]
calculate_rmse <- function(pred, target) {
rmse <- sqrt(mean((pred - target)^2))
return(rmse)
}
## =========================================fit model=========================================##
# all vars
lm_all <- lm(PRICE ~ ., data = train_d)
# select
selected_vars <- c("wheelbase", "enginesize", "boreratio", "stroke", "compressionratio", "horsepower", "peakrpm", "mpg")
formula <- as.formula(paste("PRICE ~", paste(selected_vars, collapse = " + ")))
lm_select <- lm(formula, data = train_d)
# 2-way interaction
lm_2way <- lm(PRICE ~ (.)^2, data = train_d)
# model all
pred = lm_all$fitted.values
print(calculate_rmse(pred, train_d$PRICE)) # RMSE: 2251.896
pred = predict(lm_all, test_d)
print(calculate_rmse(pred, test_d$PRICE)) # RMSE: 1876.706
# model select
pred = lm_select$fitted.values
print(calculate_rmse(pred, train_d$PRICE)) # RMSE: 2283.604
pred = predict(lm_select, test_d)
print(calculate_rmse(pred, test_d$PRICE)) # RMSE: 1834.355
# model 2-way
pred = lm_2way$fitted.values
print(calculate_rmse(pred, train_d$PRICE)) # RMSE: 946.6921
pred = predict(lm_2way, test_d)
print(calculate_rmse(pred, test_d$PRICE)) # RMSE: 10361.03
model_summary = summary(lm_all)
significant_vars <- model_summary$coefficients[, "Pr(>|t|)"] < 0.05
print("Variables with p-value < 0.05:")
print(names(significant_vars[significant_vars == TRUE]))
model_summary = summary(lm_select)
significant_vars <- model_summary$coefficients[, "Pr(>|t|)"] < 0.05
print("Variables with p-value < 0.05:")
print(names(significant_vars[significant_vars == TRUE]))
model_summary = summary(lm_2way)
significant_vars <- model_summary$coefficients[, "Pr(>|t|)"] < 0.05
print("Variables with p-value < 0.05:")
print(names(significant_vars[significant_vars == TRUE]))
## variables with p-value < 0.05: "wheelbase","enginesize","boreratio","stroke","horsepower","peakrpm"
# Calculate correlation coefficients
correlation_matrix <- cor(final_data[, c("wheelbase", "enginesize", "boreratio", "stroke", "horsepower", "peakrpm", "PRICE")])
# high correlations var: wheelbase, enginesize, horsepower
library(readr)
Titanic <- read_csv("~/Desktop/巨量課/dataset/Titanic.csv")
View(Titanic)
dataset <- read.csv("~/Desktop/巨量課/dataset/Titanic.csv")
# 2.1. [5 pts] Please convert the categorical variables into R factors or Python Pandas
# Categoricals .
#Categorical variables : Survived, Pclass, Sex, Embarked
categorical_variables <- c("Survived","Pclass","Sex","Embarked")
#using lapply() to convert those categorical variables into R factors
dataset[categorical_variables] <- lapply(dataset[categorical_variables], factor)
#see the result
str(dataset)
# 2.2. [10 pts] Calculate the number of NA values in each column with any
# “apply”functions in R or Python, and remove those records (rows) with NA values.
#get the NA position in each column (for later codes to use)
col_na <- apply(dataset,2,function(x){is.na(x)})
#get the numbers of NA values in each column and see it
col_na_numbers <- apply(col_na,2,sum)
print(col_na_numbers)
#remove rows with NA values, by the way, dataset <- na.omit(dataset) also works
#get the boolean for rows, TRUE for rows without NA values, FALSE for otherwise.
test <- apply(col_na,1,function(x){!sum(x)})
dataset <- dataset[test,]
#see if the rows had been removed successfully or not (they had been removed)
col_na_after <- apply(dataset,2,function(x){is.na(x)})
col_na_numbers_after <- apply(col_na_after,2,sum)
print(col_na_numbers_after)
str(dataset)
# 2.3. [5 pts] Calculate the average fare (‘Fare’) among the different classes (‘Pclass’).
# Please sort the average fare in ascending order and show the result.
average_fare <- aggregate(Fare ~ Pclass, FUN = mean, data = dataset)#Group fare by pclass and calculate the average
average_fare <- average_fare[order(average_fare$Fare), ]#Sort by fare from low to high
average_fare
# 2.4 correlation matrix
dataset$Survived <- as.numeric(dataset$Survived)#Convert back to numerical data
dataset$Pclass <- as.numeric(dataset$Pclass)
correlation_matrix <- cor(dataset[, c("Survived", "Pclass", "Age", "SibSp", "Parch", "Fare")])#Grab the numerical variables and make a correlation matrix
correlation_matrix
# the strongest positive correlation
diag(correlation_matrix) <- 0  #Set the diagonal to 0 first to eliminate the correlation between yourself and yourself.
#Find the position with the maximum value and put it into the matrix
max_corr <- which(correlation_matrix == max(correlation_matrix, na.rm = TRUE), arr.ind = TRUE)
#the strongest negative correlation
#Find the position of the minimum value and put it into the matrix
min_corr <- which(correlation_matrix == min(correlation_matrix, na.rm = TRUE), arr.ind = TRUE)
#print the result
cat("Strongest Positive Correlation:\n", rownames(correlation_matrix)[max_corr[1, 1]], "&", rownames(correlation_matrix)[max_corr[1, 2]])
cat("Strongest Negative Correlation:\n", rownames(correlation_matrix)[min_corr[1, 1]], "&", rownames(correlation_matrix)[min_corr[1, 2]])
##From the above results, it can be seen that Parch & SibSp is the strongest positive correlation
##Fare & Pclass are the strongest negative correlations
